![image](./images/hongsukchoi_photo_0.png){: style="float: left; width: 175px; height: 225px; margin-right: 1em; margin-top: 0.5em  "}

### **Researcher**
I am a Machine Learning / Computer Vision Researcher at Samsung AI center, New York. I received a Master's degree at Seoul National University (SNU) computer vision lab after converting my PhD program. I was advised by Prof. <a href="https://cv.snu.ac.kr/index.php/kmlee/"> Kyoung Mu Lee</a>, Editor in Chief (EIC), IEEE Trans. on PAMI (Pattern Analysis and Machine Intelligence). Please refer to my **[CV](images/CV_Hongsuk_Choi.pdf)** for more information.
Email: **redstonepo@gmail.com** 

[Google Scholar](https://scholar.google.com/citations?user=CZbowncAAAAJ&hl=en) \
[Linkedin](https://www.linkedin.com/in/hongsuk-choi-6b081a143/) \
[Github](https://github.com/hongsukchoi)

        
<br>

### **Introduction**
I am interested in a broad range of machine learning and computer vision topics. I have
in-depth experience in 3D reconstruction and neural image generation.

- Published <b>9 papers</b> at top ML/CV conferences (<b>CVPR, ECCV, ICLR</b>) in <b>2 years of my Masterâ€™s program</b>.
- I am the <b>first author on 7 papers</b> out of 11 papers.
- My work has over 600 Google Scholar citations, an H-index of 6 in 3 years.
- I maintain GitHub projects with around 1000 stars.
- I am a green card holder in the US, having received EB1 (Outstanding Researcher) approval from USCIS.

<br>

### **Publications (Selected)**


<!--FineControlNet-->
<p>
<img src="images/finecontrolnet_teaser_clearer.gif" align="left" style="width:360px; height:200px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:60px;">
<b><font color="0066CC"> FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection </font></b>
<br>
<b>Hongsuk Choi (*)</b>, Isaac Kasahara (*), Selim Engin, Moritz Alexander Graule, Nikhil
Chavan-Dafle, Volkan Isler (* equal contribution)
<br>
<b>arxiv</b>
<br>
[<a href="https://samsunglabs.github.io/FineControlNet-project-page/">WEBPAGE</a>] [<a>ARXIV</a>] [<a>CODE</a>] 
</div>
</p>
<br>
<br>
<br>




<!--HandNeRF-->
<p>
<img src="images/3drecon.gif" align="left" style="width:360px; height:200px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:60px;">
<b><font color="0066CC"> HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a Single RGB Image </font></b>
<br>
<b>Hongsuk Choi</b>, Nikhil Chavan-Dafle, Jiacheng Yuan, Volkan Isler, Hyunsoo Park
<br>
<b>arxiv</b>
<br>
[<a href="https://arxiv.org/abs/2309.07891">ARXIV</a>] [<a href="https://youtu.be/AxkIFcymwIo?si=STr244Exi6cOHG_w">VIDEO</a>] [<a href="https://github.com/SamsungLabs/HandNeRF">CODE</a>] 
</div>
</p>
<br>
<br>
<br>


<!--Rethinking-->
<p>
<img src="images/Pretrain_ICLR2023.png" align="left" style="width:360px; height:200px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:60px;">
<b><font color="0066CC"> Rethinking Self-Supervised Visual Representation Learning in Pre-training for 3D Human Pose and Shape Estimation </font></b>
<br>
<b>Hongsuk Choi (*)</b>, Hyeongjin Nam (*), Taeryung Lee, Gyeongsik Moon, Kyoung Mu Lee (* equal contribution)
<br>
In <b>ICLR 2023</b>
<br>
[<a href="https://arxiv.org/abs/2303.05370">ARXIV</a>] [<a href="https://iclr.cc/virtual/2023/poster/11695">VIDEO</a>]
</div>
</p>

<br>

<!--Three Recipes-->
<p>
<img src="images/three_recipes.png" align="left" style="width:360px; height:200px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:60px;">
<b><font color="0066CC"> Three Recipes for Better 3D Pseudo-GTs of 3D Human Mesh Estimation in the Wild </font></b>
<br>
Gyeongsik Moon, <b>Hongsuk Choi</b>, Sanghyuk Chun, Jiyoung Lee, Sangdoo Yun

<br>
In <b>CVPRW 2023</b>
<br>
[<a href="https://arxiv.org/abs/2304.04875">ARXIV</a>] [<a href="https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Moon_Three_Recipes_for_Better_3D_Pseudo-GTs_of_3D_Human_Mesh_CVPRW_2023_paper.pdf">PDF</a>] [<a href="https://github.com/mks0601/NeuralAnnot_RELEASE">HOMEPAGE</a>]
</div>
</p>

<br>



<!--MonoNHR-->
<p>
<img src="images/MonoNHR_3DV2022.gif" align="left" style="width:360px; height:180px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:60px;">

<b><font color="0066CC"> MonoNHR: Monocular Neural Human Renderer </font></b>
<br>
<b>Hongsuk Choi (*)</b>, Gyeongsik Moon (*), Matthieu Armando, Vincent Leroy, Kyoung Mu Lee, Gregory Rogez (* equal contribution)
<br>
In <b>3DV 2022</b>
<br>
[<a href="https://arxiv.org/abs/2210.00627">ARXIV</a>] [<a href="https://youtu.be/9-hfGf7dRw4">VIDEO</a>]
</div>
</p>

<br>

<!--HandOccNet-->
<p>
<img src="images/HandOccNet_CVPR2022.png" align="left" style="width:360px; height:180px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:50px;">

<b> <font color="0066CC"> HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network </font> </b>
<br>
JoonKyu Park (*), Yeonguk Oh (*), Gyeongsik Moon (*), <b>Hongsuk Choi</b>, Kyoung Mu Lee (* equal contribution)
<br>
In <b>CVPR 2022</b>
<br>
[<a href="https://arxiv.org/abs/2203.14564">ARXIV</a>] [<a href="https://github.com/namepllet/HandOccNet">CODE</a>]
</div>
</p>


<br>

<!--3DCrowdNet-->
<p>
<img src="images/3DCrowdNet_CVPR2022.png" align="left" style="width:360px; height:180px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:60px;">

<b> <font color="0066CC"> Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes </font> </b>
<br>
<b>Hongsuk Choi</b>, Gyeongsik Moon, JoonKyu Park, Kyoung Mu Lee
<br>
In <b>CVPR 2022</b>
<br>
[<a href="https://arxiv.org/abs/2104.07300">ARXIV</a>] [<a href="https://github.com/hongsukchoi/3DCrowdNet_RELEASE">CODE</a>]
</div>
</p>


<br>

<!--Hand4Whole-->
<p>
<img src="images/Hand4Whole_CVPRW2022.png" align="left" style="width:360px; height:180px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:80px;">

<b> <font color="0066CC"> Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation </font> </b>
<br>
Gyeongsik Moon, <b>Hongsuk Choi</b>, Kyoung Mu Lee
<br>
In <b>CVPRW 2022</b>
<br>
[<a href="https://arxiv.org/abs/2011.11534">ARXIV</a>] [<a href="https://youtu.be/Ym_CH8yxBso">VIDEO</a>] [<a href="https://github.com/mks0601/Hand4Whole_RELEASE">CODE</a>]
</div>
</p>


<br>

<!--NeuralAnnot-->
<p>
<img src="images/NeuralAnnot_CVPRW2022.png" align="left" style="width:360px; height:180px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:80px;">

<b> <font color="0066CC"> NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets </font> </b>
<br>
Gyeongsik Moon, <b>Hongsuk Choi</b>, Kyoung Mu Lee
<br>
In <b>CVPRW 2022</b>
<br>
[<a href="https://arxiv.org/abs/2011.11232">ARXIV</a>] [<a href="https://github.com/mks0601/NeuralAnnot_RELEASE">HOMEPAGE</a>]
</div>
</p>


<br>

<!--TCMR-->
<p>
<img src="images/TCMR_CVPR2021.gif" align="left" style="width:360px; height:180px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:50px;">

<b> <font color="0066CC"> Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video </font> </b>
<br>
<b>Hongsuk Choi</b>, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
<br>
In <b>CVPR 2021</b>
<br>
[<a href="https://arxiv.org/abs/2011.08627">ARXIV</a>] [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_Beyond_Static_Features_for_Temporally_Consistent_3D_Human_Pose_and_CVPR_2021_paper.pdf">PDF</a>] [<a href="https://github.com/hongsukchoi/TCMR_RELEASE">CODE</a>] [<a href="https://www.youtube.com/watch?v=WB3nTnSQDII">VIDEO</a>]
</div>
</p>


<br>

<!--Pose2Mesh-->
<p>
<img src="images/Pose2Mesh_ECCV2020.png" align="left" style="width:360px; height:200px; margin-right:10px;  vertical-align=middle;">
<div style="margin-bottom:100px;">

<b> <font color="0066CC"> Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose </font> </b>
<br>
<b>Hongsuk Choi (*)</b>,  Gyeongsik Moon (*) , Kyoung Mu Lee (* equal contribution)
<br>
In <b>ECCV 2020</b>
<br>
[<a href="https://arxiv.org/abs/2008.09047">ARXIV</a>] [<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520749.pdf">PDF</a>] [<a href="https://github.com/hongsukchoi/Pose2Mesh_RELEASE">CODE</a>] [<a href="https://youtu.be/utaHeByNauc">VIDEO</a>]
</div>
</p>


<br>    


### **Honors**

<p align="middle">
<img src="images/Master_dissertation_award.jpg" width="160px" height="240px">
</p>
<p align="center">
<b>SNU Distinguished Master Dissertation Award</b> (2022)
</p>

<p align="middle">
<img src="images/3DPW_challenge_1st.png" width="240px" height="180px"><img src="images/3DPW_challenge_2nd.png" width="240px" height="180px">
</p>
<p align="center">
  <b>1st place</b> and <b>2nd place</b> at the <b>3D human pose estimation in the wild (3DPW) challenge</b> of a without association track in joint orientation and position metrics, respectively <br> (workshop in conjunction with <b>ECCV 2020</b>)
</p>

<p align="middle">
<img src="images/Qualcomm.png" width="225px" height="80px">
</p>
<p align="center">
<b>Qualcomm IT Tour 2019 presentation competition 1st place</b> (2019)
</p>

<br>

### **Service**
* A Reviewer for computer vision and machine learning conferences 
\- <b>CVPR</b>, <b>ICCV</b>, <b>ECCV</b>, <b>NeuRIPS</b> and others
* A Reviewer for computer vision journals
\- <b>TPAMI</b>, IEEE TIP and others

<br>

### **Experience**
<p align="middle">
<img src="images/SAIC-NY.png" width="50%" height="50px">
</p>
<p align="center">
  <b>ML/CV Researcher</b>, <b>Samsung Research America</b>, New York, NY, USA <br> (Jun. 2022 - present)
</p>

<p align="middle">
<img src="images/Naver.png" width="280px" height="50px">
</p>
<p align="center">
  <b>Visiting Researcher</b>, <b>Naver CLOVA AI Lab</b>, Seoul, Korea <br> (Mar. 2022 - May. 2022)
</p>

<p align="middle">
<img src="images/NLE.jpg" width="230px" height="50px">
</p>
<p align="center">
  <b>Research Intern</b>, <b>Naver Labs Europe (NLE)</b>, Grenoble, France <br> (April. 2021 - Oct. 2021)
</p>


<p align="middle">
<img src="images/SNU.png" width="100px" height="100px">
</p>
<p align="center">
  <b>M.S.</b>, ECE, <b>Seoul National University (SNU)</b>, Seoul, Korea <br> (Mar. 2020 - Feb. 2022)
</p>


<p align="middle">
<img src="images/roka.png" swidth="300px" height="100px">
</p>
<p align="center">
  <b>Repulic of Korea Airforce (ROKA)</b>, Gyeryong, Korea <br> (Mar. 2015 - Mar. 2017)
</p>

<p align="middle">
<img src="images/SNU.png" width="100px" height="100px">
</p>
<p align="center">
  <b>B.S.</b>, Business, CSE, <b>Seoul National University (SNU)</b>, Seoul, Korea <br> (Mar. 2013 - Feb. 2020)
</p>
